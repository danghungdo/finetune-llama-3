# General
name: "domain router"

# Dataset
data_path: "data/dataset.pkl"

# Model
model_name: "/home/dang.hung.do/models/meta-llama/Meta-Llama-3-8B-Instruct"
model:
  # quantization config
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4" 
  bnb_4bit_use_double_quant: True
  bnb_4bit_compute_dtype: "torch.bfloat16"
  # lora config
  r: 8
  lora_alpha: 16
  target_modules: ["q_proj", "v_proj"]
  lora_dropout: 0.05
  bias: "none"
  task_type: "SEQ_CLS"
# training config
train:
  output_dir: "sequence_classification"
  learning_rate: 0.0001
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  num_train_epochs: 5
  weight_decay: 0.01
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  load_best_model_at_end: True
  max_len: 512

  
